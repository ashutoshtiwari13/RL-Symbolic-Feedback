{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK9NrucTQK_S",
        "outputId": "2a82b0bc-97fe-43e5-ee1f-228c7e5cbbbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/318.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/105.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets trl -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cIlQUxbzQP8G"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xb0l9XiXQUQd"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/ashutoshtiwari13/trl.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rz-gQfbtQVve"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = '<hidden>'\n",
        "\n",
        "#wandb token = <hidden>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7qGKeCJfQXWN"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/trl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN_bKcXmQY_u",
        "outputId": "07d4bbc0-1d9e-41eb-a358-9e195f9e3cb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/__init__.py\n"
          ]
        }
      ],
      "source": [
        "import trl\n",
        "print(trl.__file__ )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J_-ASOKEQaeX"
      },
      "outputs": [],
      "source": [
        "# %cd /content/trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "goBKwicPQa4O"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_C5X1_HsQcQj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from trl import PPOConfig, PPOTrainer,AutoModelForCausalLMWithValueHead\n",
        "from trl.core import LengthSampler\n",
        "import numpy as np\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "d_pzrIynRvpU"
      },
      "outputs": [],
      "source": [
        "class SymbolicEnvironment:\n",
        "    def __init__(self, tokenizer, sentiment_analyzer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sentiment_analyzer = sentiment_analyzer\n",
        "\n",
        "    def analyze_sentiment(self, text: str) -> float:\n",
        "        return self.sentiment_analyzer(text)[0]['score']\n",
        "\n",
        "    def get_vectorized_feedback(self, prompt: str, response: str) -> List[float]:\n",
        "        full_text = prompt + response\n",
        "        tokens = self.tokenizer.tokenize(full_text)\n",
        "        sentiment_score = self.analyze_sentiment(full_text)\n",
        "\n",
        "        feedback = [sentiment_score if sentiment_score > 0.5 else 1 - sentiment_score] * len(tokens)\n",
        "        return feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "M-iU7duZR2NW"
      },
      "outputs": [],
      "source": [
        "class RLSFTrainer:\n",
        "    def __init__(self, config: PPOConfig, model_name: str, symbolic_env: SymbolicEnvironment):\n",
        "        self.config = config\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
        "        self.ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
        "        self.symbolic_env = symbolic_env\n",
        "        self.ppo_trainer = PPOTrainer(config, self.model, self.ref_model, self.tokenizer)\n",
        "\n",
        "    def train(self, dataset, num_epochs: int):\n",
        "        for epoch in range(num_epochs):\n",
        "            for batch in dataset:\n",
        "                query_tensors = batch[\"input_ids\"]\n",
        "                # attention_mask = batch[\"attention_mask\"]\n",
        "                labels = batch[\"label\"]\n",
        "\n",
        "                print(\"Debug: query_tensors shape:\", query_tensors.shape)\n",
        "                # print(\"Debug: attention_mask shape:\", attention_mask.shape)\n",
        "\n",
        "                # Convert query_tensors to a list of 1D tensors\n",
        "                query_tensors_list = [tensor for tensor in query_tensors]\n",
        "                print(\"Debug: query_tensors_list length:\", len(query_tensors_list))\n",
        "                print(\"Debug: first tensor in list shape:\", query_tensors_list[0].shape)\n",
        "\n",
        "                # Prepare inputs for generate\n",
        "                generate_kwargs = {\n",
        "                    # \"attention_mask\": attention_mask,\n",
        "                    \"max_new_tokens\": 20,\n",
        "                    \"do_sample\": True,\n",
        "                    \"top_k\": 0,\n",
        "                    \"top_p\": 1.0,\n",
        "                    \"temperature\": 1.0\n",
        "                }\n",
        "\n",
        "                try:\n",
        "                    response_tensors = self.ppo_trainer.generate(query_tensors_list, **generate_kwargs)\n",
        "                    print(\"Debug: generate successful\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Debug: generate failed with error: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                print(\"Debug: response_tensors type:\", type(response_tensors))\n",
        "                if isinstance(response_tensors, list):\n",
        "                    print(\"Debug: response_tensors list length:\", len(response_tensors))\n",
        "                    print(\"Debug: first response tensor shape:\", response_tensors[0].shape)\n",
        "                else:\n",
        "                    print(\"Debug: response_tensors shape:\", response_tensors.shape)\n",
        "\n",
        "                prompts = self.tokenizer.batch_decode(query_tensors, skip_special_tokens=True)\n",
        "                responses = self.tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
        "\n",
        "                rewards = []\n",
        "                for prompt, response, label in zip(prompts, responses, labels):\n",
        "                    feedback = self.symbolic_env.get_vectorized_feedback(prompt, response)\n",
        "                    rewards.append(torch.tensor(feedback))\n",
        "\n",
        "                # Ensure rewards match the shape of response_tensors\n",
        "                max_len = max(len(r) for r in rewards)\n",
        "                rewards = [torch.nn.functional.pad(r, (0, max_len - len(r)), value=r[-1]) for r in rewards]\n",
        "                rewards = torch.stack(rewards)\n",
        "\n",
        "                print(\"Debug: rewards shape:\", rewards.shape)\n",
        "\n",
        "                # Convert response_tensors to a tensor if it's a list\n",
        "                if isinstance(response_tensors, list):\n",
        "                    response_tensors = torch.stack(response_tensors)\n",
        "\n",
        "                try:\n",
        "                    self.ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "                    print(\"Debug: PPO step successful\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Debug: PPO step failed with error: {str(e)}\")\n",
        "\n",
        "            print(f\"Epoch {epoch + 1} completed\")\n",
        "\n",
        "    def evaluate(self, dataset) -> Dict[str, float]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in dataset:\n",
        "            query_tensors = batch[\"input_ids\"]\n",
        "            # attention_mask = batch[\"attention_mask\"]\n",
        "            labels = batch[\"label\"]\n",
        "\n",
        "            # Convert query_tensors to a list of 1D tensors\n",
        "            query_tensors_list = [tensor for tensor in query_tensors]\n",
        "\n",
        "            # Prepare inputs for generate\n",
        "            generate_kwargs = {\n",
        "                # \"attention_mask\": attention_mask,\n",
        "                \"max_new_tokens\": 20,\n",
        "                \"do_sample\": False\n",
        "            }\n",
        "\n",
        "            response_tensors = self.ppo_trainer.generate(query_tensors_list, **generate_kwargs)\n",
        "\n",
        "            prompts = self.tokenizer.batch_decode(query_tensors, skip_special_tokens=True)\n",
        "            responses = self.tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
        "\n",
        "            for prompt, response, label in zip(prompts, responses, labels):\n",
        "                sentiment_score = self.symbolic_env.analyze_sentiment(prompt + response)\n",
        "                predicted_label = 1 if sentiment_score > 0.5 else 0\n",
        "                if predicted_label == label.item():\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "        accuracy = correct / total\n",
        "        return {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6nWP2mRTImy",
        "outputId": "1a848032-e182-4d4e-ee13-a48be0bfc8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug: Dataset preparation complete\n",
            "Debug: Dataset columns: ['label', 'input_ids']\n",
            "Debug: Dataset features: {'label': ClassLabel(names=['neg', 'pos'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)}\n",
            "Debug: Shape of first item:\n",
            "  label: torch.Size([])\n",
            "  input_ids: torch.Size([512])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['label', 'input_ids'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prepare_dataset(dataset, tokenizer, max_length=512):\n",
        "    def tokenize_and_truncate(examples):\n",
        "        tokenized = tokenizer(\n",
        "            examples['text'],\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        # Remove attention_mask from tokenized output\n",
        "        tokenized.pop('attention_mask', None)\n",
        "        # Add the label to the tokenized output\n",
        "        tokenized['label'] = examples['label']\n",
        "        return tokenized\n",
        "\n",
        "    # Keep the 'label' column when removing columns\n",
        "    columns_to_remove = [col for col in dataset.column_names if col not in ['label']]\n",
        "    dataset = dataset.map(tokenize_and_truncate, batched=True, remove_columns=columns_to_remove)\n",
        "\n",
        "    # Set the format for all columns including 'label', but excluding attention_mask\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "\n",
        "    print(\"Debug: Dataset preparation complete\")\n",
        "    print(\"Debug: Dataset columns:\", dataset.column_names)\n",
        "    print(\"Debug: Dataset features:\", dataset.features)\n",
        "\n",
        "    # Print shape of first item in dataset\n",
        "    first_item = dataset[0]\n",
        "    print(\"Debug: Shape of first item:\")\n",
        "    for key, value in first_item.items():\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            print(f\"  {key}: {value.shape}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {type(value)}\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "train_dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "train_dataset = prepare_dataset(train_dataset, tokenizer)\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "e5oYx6tzR65K"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Load dataset\n",
        "    train_dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
        "    test_dataset = load_dataset(\"imdb\", split=\"test[:1000]\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Prepare datasets\n",
        "    train_dataset = prepare_dataset(train_dataset, tokenizer)\n",
        "    test_dataset = prepare_dataset(test_dataset, tokenizer)\n",
        "\n",
        "    # Initialize config\n",
        "    config = PPOConfig(\n",
        "        model_name=\"gpt2\",\n",
        "        learning_rate=1e-5,\n",
        "        batch_size=4,\n",
        "        mini_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "    )\n",
        "\n",
        "    # Initialize symbolic environment\n",
        "    from transformers import pipeline\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0 if torch.cuda.is_available() else -1)\n",
        "    symbolic_env = SymbolicEnvironment(tokenizer, sentiment_analyzer)\n",
        "\n",
        "    # Initialize and train RLSF model\n",
        "    rlsf_trainer = RLSFTrainer(config, \"gpt2\", symbolic_env)\n",
        "    rlsf_trainer.train(train_dataset, num_epochs=5)\n",
        "\n",
        "    # Evaluate RLSF model\n",
        "    rlsf_results = rlsf_trainer.evaluate(test_dataset)\n",
        "    print(\"RLSF Results:\", rlsf_results)\n",
        "\n",
        "    # Initialize and train standard RL model (without symbolic feedback)\n",
        "    standard_config = PPOConfig(\n",
        "        model_name=\"gpt2\",\n",
        "        learning_rate=1e-5,\n",
        "        batch_size=4,\n",
        "        mini_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "    )\n",
        "    standard_trainer = RLSFTrainer(standard_config, \"gpt2\", None)\n",
        "    standard_trainer.train(train_dataset, num_epochs=5)\n",
        "\n",
        "    # Evaluate standard RL model\n",
        "    standard_results = standard_trainer.evaluate(test_dataset)\n",
        "    print(\"Standard RL Results:\", standard_results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
